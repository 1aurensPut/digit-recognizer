---
title: "The Digit Recognizer Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

# The project

In collaboration with Optimistic Data Spirit, this is my process in R to solve the Digit Recognizer Project Kaggle competition and hence to learn computer vision fundamentals. For all details about this competition see <https://www.kaggle.com/c/digit-recognizer/data>.

## The data

The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems (Wikipedia). Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels. Each pixel has a single pixel-value between 0 and 255 associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker (Kaggle website).

We received three csv files: 

* train.csv - the training dataset with 785 columns. The first column, called "label", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image. Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).

* test.csv - same as the training set, except that it does not contain the "label" column.

* sample-submission.csv - for each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict.

## The evaluation

The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.

# My approach

```{r, include = FALSE}
# clean memory
rm(list=ls())
# call libraries
library(tidyverse)
library(keras)
```

Watching the Deep Learning with R in Motion: the MNIST dataset video from Manning publication, I took the following notes: 

This is a single label multi-class (10 options) classification (trying to predict a categorical variable) problem. 
This is a computer vision problem for which we can use a Convolutional Network (CNN).

The three phases of machine learning projects are: 

1. Data preparation
    + Obtain
    + Rearrange
    + Normalize
    + Reformat
2. Model definition
    + I
    + I
3. Evaluation
    + Item 3a
    + Item 3b

Steps to solve this problem include:

* Normalizing the values so that each pixel is represented by a value between 0 and 1 (as opposed to 0 and 255)

## Data preparation

Note that the data received from Kaggle is in a different format than the video I am following.

### Obtain data

I obtained my dataset from the Kaggle competition. 

```{r, include = FALSE}
training <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/train.csv")
testing <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/test.csv")

# brief visualisation of the datasets
dim(training)
dim(testing)
```

### Rearrange data

We want a 2D tensor (like a data.frame or classical R matrix). This was done by Kaggle for us.

### Normalize the values

Normalizing the values so that each pixel is represented by a value between 0 and 1 (as opposed to 0 and 255). This is done by simply dividing the value of each pixel by 255.

```{r}
train_data <- training[,2:785]/255
train_labels <- training[,1]
test_data <- testing/255

#Change dataset to matrix
train_data <- data.matrix(train_data)
test_data <- data.matrix(test_data)
```

So we now have two 2D normalized tensors (close to a matrix).

Tensor: 

rank: # of dimension
data type: typeof() in R
shape: dim() in R

### Reformat the labels

Need to format the labels as the data (to categorical), aka one hot encoding.
```{r}
train_labels <- to_categorical(train_labels)
```

## Building a model

### Simple neural network

Building a neural network:
1 - define network of architecture building layers
2 - compile the network (with 2 functions being the optimization and the loss function)
3 - train the model with the fit function

```{r architecture}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
  # units is the number of neurons in a layer
  summary(network)
```

### Compile the model

Add specific function to tell tensor flow which one to use.
The loss function is what we want to minimize.
The metrics will be tracked and is the function we want to maximize.

```{r compile}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

There is no need save this into a new object since the compile function update the object, here being network.

### Train the model

Provide the label as the desired output (this is supervised learning).

```{r train}
history <- network %>%
  fit(train_data, train_labels, epochs = 5, batch_size = 128)
```

### Evaluate the model

```{r evaluate}
# this barfs as we did not split the dataset in two sets
metrics <- network %>%
  evaluate(test_data, test_labels)
metrics
```

### Predict the test values

```{r predict}
Labels <- network %>% predict_classes(test_data)
Labels
```


### Save predictions to submission format

```{r submission format}
# needs to be two columns: ImageID, Label
ImageID <- c(1:28000)
Label <- Labels

df <- data.frame(ImageID, Label)

path <- "~/projects/kaggle/digit-recognizer/data/processed/submission.csv"

write_csv(df, path)
```




## Useful resources

https://www.youtube.com/watch?v=K6e8WnJeivQ
https://www.kaggle.com/wwu651/cnn-keras-with-r


