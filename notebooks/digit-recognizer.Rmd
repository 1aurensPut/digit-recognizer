---
title: "The Digit Recognizer Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

# The project

In collaboration with the Optimistic Data Spirit, this is my process in R to solve the Digit Recognizer Project Kaggle competition to learn about computer vision fundamentals. For all details about this competition see <https://www.kaggle.com/c/digit-recognizer/data>.

## The data

As specified on the Kaggle website, The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems (Wikipedia). Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels. Each pixel has a single pixel-value between 0 and 255 associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker.

We received three csv files: 

* train.csv - the training dataset with 785 columns. The first column, called "label", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image. Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).

* test.csv - same as the training set, except that it does not contain the "label" column.

* sample-submission.csv - for each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict.

The format in which the data is provided on the Kaggle plateform is already reshaped from the initial 28 x 28 pixel images. So no need to do this step.

The data provided on the Kaggle website is also different than the original MNIST dataset in the sense that only 42000 of the original 60000 training set where provided. To test the accuracy of the mmodel to be build, one can set aside 20% of the training dataset to test before submitting to the website for evaluation.

## The evaluation

The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.
  
# My approach

Instructed by the Deep Learning with R in Motion: the MNIST dataset video from Manning publication, I took the following notes.

The three phases of machine learning projects are:

1. Data preparation
    + Obtain
    + Rearrange
    + Normalize
    + Reformat
2. Model definition (building the model)
    + define network of architecture building layers
    + compile the network (with 2 functions being the optimization and the loss function)
    + train the model with the fit function
3. Evaluation
    + Predict labels of the test dataset
    + Confusion matrix
    + Determine accuracy
    
Once the model is trained and the accuracy has been tested, we can use it to make predictions.

This is a single label multi-class (10 options) classification (trying to predict a categorical variable) problem. This is a computer vision problem for which we can use a neural network.

## Data preparation

Note that the data received from Kaggle is in a different format than the video I am following. The original MNIST dataset is provided as 28 x28 pixel images that have to be reshaped to be used in the CNN. 

```{r, include = FALSE}
# clean memory
rm(list=ls())
# call libraries
library(tidyverse)
library(keras)
```

## Data preparation

Note that the data received from Kaggle is in a different format than the video I am following. The original MNIST dataset is provided as 28 x28 pixel images that have to be reshaped to be used in the CNN. 
### Obtain data

I obtained my dataset from the Kaggle competition. 

```{r, include = FALSE}
train_dataset <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/train.csv")
predict_dataset <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/test.csv")

# brief visualisation of the datasets
dim(train_dataset)
dim(predict_dataset)
```

Good practice is to take the dataset and split it two datasets (80% for the training of the model and 20% for the testing of the model). However, the datasets provided by Kaggle are named train.csv and test.csv. If I understand this correctly, the train.csv needs to be split into train_train and train_test datasets. The test dataset is the one for which we need to provide predictions.

```{r}
sub80 <- sort(sample(nrow(train_dataset), nrow(train_dataset)* 0.8))
train_dataset_train <- train_dataset[sub80,]
train_dataset_test <- train_dataset[-sub80,]

# brief visualisation of the datasets
dim(train_dataset_train)
dim(train_dataset_test)
```

### Rearrange data

We want a 2D tensor (like a data.frame or classical R matrix). This was done by Kaggle for us.

### Normalize the values

Normalizing the values so that each pixel is represented by a value between 0 and 1 (as opposed to 0 and 255). This is done by simply dividing the value of each pixel by 255.

```{r}
train_dataset_train_normalized_features <- train_dataset_train[,2:785]/255
train_dataset_train_labels <- train_dataset_train[,1]

train_dataset_test_normalized_features <- train_dataset_test[,2:785]/255
train_dataset_test_labels <- train_dataset_test[,1]

predict_normalized_dataset <- predict_dataset/255

#Change dataset to matrix
train_dataset_train_normalized_features <- data.matrix(train_dataset_train_normalized_features)
train_dataset_train_labels <- data.matrix(train_dataset_train_labels)

train_dataset_test_normalized_features <- data.matrix(train_dataset_test_normalized_features)
train_dataset_test_labels <- data.matrix(train_dataset_test_labels)

predict_normalized_dataset <- data.matrix(predict_normalized_dataset)
```

So we now have two 2D normalized tensors (similar, but not the same as a matrix).

### Reformat the labels

Need to format the labels as the data (to categorical), aka one hot encoding.
```{r}
train_dataset_train_labels <- to_categorical(train_dataset_train_labels)
train_dataset_test_labels <- to_categorical(train_dataset_test_labels)
```

## Model definition (building a model)

### Simple neural network

Building a neural network:
1 - define network of architecture building layers
2 - compile the network (with 2 functions being the optimization and the loss function)
3 - train the model with the fit function

```{r architecture}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
  # units is the number of neurons in a layer
  summary(network)
```

### Compile the model

Add specific function to tell tensor flow which one to use.
The loss function is what we want to minimize.
The metrics will be tracked and is the function we want to maximize.

```{r compile}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

There is no need save this into a new object since the compile function update the object, here being the network.

### Train the model

Provide the label as the desired output (this is supervised learning).

```{r train}
history <- network %>%
  fit(train_dataset_train_normalized_features, train_dataset_train_labels, epochs = 5, batch_size = 100)
```

### Evaluate the model

```{r evaluate}
metrics <- network %>%
  evaluate(train_dataset_test_normalized_features, train_dataset_test_labels)
metrics
```

### Predict the test values

```{r predict}
predicted_labels <- network %>% predict_classes(predict_normalized_dataset)
predicted_labels
```


### Save predictions to submission format

```{r submission format}
# needs to be two columns: ImageID, Label
ImageID <- c(1:28000)
Label <- predicted_labels

df <- data.frame(ImageID, Label)

path <- "~/projects/kaggle/digit-recognizer/data/processed/submission.csv"

write_csv(df, path)
```

## Useful resources

https://www.youtube.com/watch?v=K6e8WnJeivQ
https://www.kaggle.com/wwu651/cnn-keras-with-r
Deep Learning with R, Francois Chollet

