---
title: "The Digit Recognizer Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

# The project

In collaboration with Optimistic Data Spirit, this is my process in R to solve the Digit Recognizer Project Kaggle competition and hence to learn computer vision fundamentals. For all details about this competition see <https://www.kaggle.com/c/digit-recognizer/data>.

## The data

The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems (Wikipedia). Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels. Each pixel has a single pixel-value between 0 and 255 associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker (Kaggle website).

We received three csv files: 

* train.csv - the training dataset with 785 columns. The first column, called "label", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image. Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).

* test.csv - same as the training set, except that it does not contain the "label" column.

* sample-submission.csv - for each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict.

## The evaluation

The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.

# My approach

```{r, include = FALSE}
# clean memory
rm(list=ls())
# call libraries
library(keras)
```

Watching the Deep Learning with R in Motion: the MNIST dataset video from Manning publication, I took the follwing notes: 

This is a single label multi-class (10 options) classification (trying to predict a categorical variable) problem. 
This is a computer vision problem for which we can use a Convolutional Network (CNN).

The three phases of machine learning projects are: 

1. Data preparation
    + Obtain
    + Rearrange
    + Normalize
    + Reformat
2. Model definition
    + I
    + I
3. Evaluation
    + Item 3a
    + Item 3b

Steps to solve this problem include:

* Normalizing the values so that each pixel is represented by a value between 0 and 1 (as opposed to 0 and 255)

## Data preparation

Note that the data received from Kaggle is in a different format than the video I am following.

### Obtain data

I obtained my dataset from the Kaggle competition. 

```{r, include = FALSE}
train.data <- data <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/train.csv")
test.data <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/test.csv")
```

```{r}
dim(train.data)
dim(test.data)
```

### Rearrange data

Want a 2D tensor (like a data.frame or classical R matrix). This was done by Kaggle for us.

### Normalize the values

Normalizing the values so that each pixel is represented by a value between 0 and 1 (as opposed to 0 and 255). This is done by simply dividing the value of each pixel by 255.

```{r}
norm.train.data <- train.data/255
#norm.train.data$label <- norm.train.data*255

norm.test.data <- train.data/255
```

So we now have two 2D normalized tensors (close to a matrix).

Tensor: 

rank: # of dimension
data type: typeof() in R
shape: dim() in R

### Reformat the labels

```{r}
labels <- train.data$label
train_labels <- to_categorical(labels)
```

## Building a model

### Simple neural network

Building a neural network:
1 - define network of architecture building layers
2 - compile the network (with 2 functions being the optimization and the loss function)
3 - train the model with the fit function

```{r architecture}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
  # units is the number of neurons in a layer
  summary(network)
```

### Compile the model

Add specific function to tell tensor flow which one to use.
The loss function is what we want to minimize.
The metrics will be tracked and is the function we want to maximize.

```{r compile}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

There is no need save this into a new object since the compile function update the object, here being network.

### Train the model

Provide the label as the desired output (this is supervised learning).

```{r train}
history <- network %>%
  # fit(train_images, train_labels, epochs = 5, batch_size = 128)
  fit(norm.train.data, norm.train.data$labels, epochs = 5, batch_size = 128)
```



## Useful resources

https://www.youtube.com/watch?v=K6e8WnJeivQ

